{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"top\"></div> \n",
    "# Table of contents\n",
    "* <a href='#Submission-instructions'>Submission instructions</a>\n",
    "* <a href='#HW2-Description'>Description of the HW2</a>\n",
    "* <a href=\"#The-plan:\">The start of this homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "\n",
    "* Deadline: see Sakai announcement.\n",
    "* Submit your homework (.ipynb file) via Sakai. Do not email.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Description\n",
    "\n",
    "In this homework assignment, we are working with instagram photos of five national parks. The task is to train up a classifier that correctly predicts the park from a photo. This is a non-trivial task, since naive approach -- using each pixel as a feature -- will fail. To see why consider the two photos below.\n",
    "\n",
    "| Arches 1| Arches 2 |\n",
    "|-|-|\n",
    "|![](./images_5_classes/archesnationalpark/2_1342855034455221159.jpg)|![](./images_5_classes/archesnationalpark/2_1343181954723713884.jpg)|\n",
    "\n",
    "Landmark occurs in both photos but not in the same position in the image. Further due to the opening in the arch, middle of the photo may be occupied by sky or rock. At best, treating pixels as feature we can analyze image in terms of color frequencies -- how intense are the red,green and blue channel.\n",
    "\n",
    "Instead of using each pixel as a feature, we provide you with state-of-art features -- computed by a neural network -- which aggregate information about edges, shapes, and color intensities across the whole image. \n",
    "\n",
    "Hence, your task is to train a prediction model which takes distilled representation of the photos, 1000 features from AlexNet, and predicts where the photo was taken. We consider only five locations in this assignment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs and the corresponding national parks:\n",
      "{0: b'greatsanddunesnationalpark', 1: b'petrifiedforestnationalpark', 2: b'archesnationalpark', 3: b'congareenationalpark', 4: b'katmainationalpark'}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import gzip\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "\n",
    "features, labels, sample_ids, label_names =  pickle.load( gzip.open( 'hw2_data.pgz', 'rb' ), **kwargs )\n",
    "print('IDs and the corresponding national parks:')\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into 403 training and 101 test examples.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "N = features.shape[0]\n",
    "arr = np.arange(N)\n",
    "np.random.shuffle(arr)\n",
    "train_num = int(round(N*0.8))\n",
    "test_num = features.shape[0]-train_num\n",
    "\n",
    "train_subset = arr[:train_num]\n",
    "train_features = features[train_subset,:]\n",
    "train_labels = labels[train_subset]\n",
    "train_sample_ids = [sample_ids[i] for i in train_subset]\n",
    "\n",
    "test_subset = arr[train_num:]\n",
    "test_features = features[test_subset,:]\n",
    "test_labels = labels[test_subset]\n",
    "test_sample_ids = [sample_ids[i] for i in test_subset]\n",
    "print(\"Dataset split into {} training and {} test examples.\".format(train_num,test_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan:\n",
    "\n",
    "We will model features in the dataset using Bayesian networks, one for each class -- natural park. \n",
    "$$\n",
    "p(\\mathbf{x} \\mid h=c) = \\prod_{(j,k) \\in \\textrm{Edges}^c} p(x_j \\mid x_k,\\theta_{j}^c)\n",
    "$$\n",
    "where $\\textrm{Edges}^c$ is the edge set for tree in class $c$ and $\\theta_{j}^c$ are parameters for conditional probability of $x_j$ given its parent $x_k$. Note that different classes will correspond to different trees and use different parameters.\n",
    "\n",
    "The tasks we will have to accomplish are:\n",
    "1. <a href='#1.-Learning-a-tree-structure-(1pt)'>Implement learning of the tree structure for each class using Chow-Liu algorithm (**1pt**) </a>\n",
    "2. <a href='#2.-Learning-parameters-of-a-tree-structured-Bayes-net-(3pt)'>Learn parameters for conditional probabilities associated with edges in these trees (**3pts**)</a>\n",
    "3. <a href=\"#3.-Compute-probability-of-a-feature-vector-x-given-a-Bayes-net's-structure-and-parameters-(2pt)\">Compute probability of a feature vector $\\mathbf{x}$ in each of the five Bayesian networks (**2pt**) </a>\n",
    "4. <a href='#4.-Compute-probabilities-that-a-feature-vector-x-belongs-to-each-class-(2pt)'>Compute probabilities that the feature vector $\\mathbf{x}$  belongs to each class (**2pt**) </a>\n",
    "5. <a href='#5.-Make-predictions-based-on-probabilities-(1pt)'>Make predictions based on probabilities (**1pt**)</a>\n",
    "6. <a href='#6.-Explore-prediction-performance-(1pt)'>Explore prediction performance (**1pt**) </a>\n",
    "\n",
    "You can get at most 10 points on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable Name | Meaning |\n",
    "|-|-|\n",
    "| {```train```&#124;```test```}```features``` | Matrix of features of photos |\n",
    "| {```train```&#124;```test```}```labels```  | Vector of labels of photos |\n",
    "| {```train```&#124;```test```}```sample_ids``` | List of ids for each sample (filenames) |\n",
    "| ```label_names``` | Names of classes (list of strings) |\n",
    "| ```class_edges``` | Dictionary of edge lists. One edge list per class |\n",
    "| ```class_thetas``` | Dictionary of parameters. One theta per class |\n",
    "\n",
    "| Function name | Purpose |\n",
    "|-|-|\n",
    "|```corr``` | Computes Pearson correlation between two vectors. |\n",
    "| ```mutual_information``` | Computes mutual information between two vectors. |\n",
    "|```mutual_info_all```| Compute mutual information between columns of  a matrix. |\n",
    "|```chowliu``` | Chow-Liu algorithm on a feature matrix. <br>Returns adjacency matrix; non-zero entry indicates edge. |\n",
    "|```get_edge_list``` | Creates an edge list from adjacency matrix. <br>Each edge represented by (parent,child) element. |\n",
    "|```get_label_subsets``` | Get sample indices for each class. Returns a dictionary of lists. <br \\>Key is class label $c$, value is sample indices for that class |\n",
    "|```get_edges_for_each_class```| Produces an edge list for each class. Returns a dictionary of lists.<br>Key is class label $c$, value is edge set for that class $\\textrm{Edges}^c$|\n",
    "|```get_thetas_for_each_class```| Learns parameters for each class. Returns a dictionary of matrices. <br>Key is class label $c$, value is matrix $\\theta^c$.| \n",
    "|```p_h_given_x_theta```| Computes probability that particular sample $x$ came from a class $h$ <br>given all the class edge sets and parameters. Uses Bayes rule.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning a tree structure (1pt)\n",
    "Chow-Liu structure learning algorithm uses mutual information to find the best tree. \n",
    "\n",
    "### Computing mutual information\n",
    "\n",
    "In class, we specified how to compute mutual information between variables distributed according to a categorical distribution. If we assume that the variables as are Gaussian distributed then mutual information is given \n",
    "by\n",
    "$$ \n",
    "I(X,Y) = -\\frac{1}{2} \\ln(1-Corr(X, Y)^2) \n",
    "$$\n",
    "where Corr denotes Pearson correlation. \n",
    "\n",
    "**ToDo**\n",
    "* Implement mutual information computation in function ```mutual_information```. [Hint: We suggest you use our ```corr``` function.]\n",
    "* Implement a function ```mutual_info_all``` which computes mutual information between all features -- columns -- in a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information 5.067 between variables with correlation 1.000.\n",
      "Mutual information 5.037 between variables with correlation -1.000.\n",
      "[[0.         5.06715848 5.03662491]\n",
      " [0.         0.         4.79462788]\n",
      " [0.         0.         0.        ]]\n",
      "MI(x,z) = 5.037 should be larger than MI(y,z) = 4.795.\n"
     ]
    }
   ],
   "source": [
    "def corr(x,y):\n",
    "    if x.shape[0] == 0:\n",
    "        rowvar = 1\n",
    "    else:\n",
    "        rowvar = 0\n",
    "    return np.corrcoef(np.asarray(x),np.asarray(y),rowvar = rowvar)[0,1]\n",
    "\n",
    "def mutual_information(x,y):\n",
    "    return -0.5 * np.log(1 - corr(x, y)**2)\n",
    "\n",
    "def mutual_info_all(M):\n",
    "    f_num = M.shape[1] #feature number\n",
    "    mi_ary = np.zeros( (f_num, f_num) )\n",
    "    for i in range(f_num):\n",
    "        for j in range(i+1, f_num):\n",
    "            mi_ary[i,j] = mutual_information(M[:,i],M[:,j])\n",
    "    \n",
    "    return mi_ary\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(10)\n",
    "y = x + 0.01*np.random.randn(10)\n",
    "z = -x + 0.01*np.random.randn(10)\n",
    "\n",
    "print((\"Mutual information {:1.3f} between variables \"+\n",
    "      \"with correlation {:1.3f}.\").format(mutual_information(x,y),\n",
    "                                     corr(x,y)))\n",
    "print((\"Mutual information {:1.3f} between variables \"+\n",
    "      \"with correlation {:1.3f}.\").format(mutual_information(x,z),\n",
    "                                     corr(x,z)))\n",
    "features = np.asmatrix([x,y,z]).transpose()\n",
    "MI = mutual_info_all(features)\n",
    "print(MI)\n",
    "print((\"MI(x,z) = {:1.3f} should be larger \" + \n",
    "       \"than MI(y,z) = {:1.3f}.\").format(MI[0,2],MI[1,2]))\n",
    "assert(MI[0,2]>MI[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything correctly, highly correlated variables, ```x``` and ```y```, should have high mutual information. Similarly highly anti-correlated variables, ```z``` and ```x```, should have high mutual information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chow-liu tree algorithm\n",
    "\n",
    "Given a feature matrix, matrix of mutual information ```M``` between features provides weights for each candidate edge in a Bayesian network. Running maximum spanning tree on the matrix of weights yields optimal connectivity structure of the graph. \n",
    "\n",
    "**ToDo**\n",
    "\n",
    "Implement Chow-Liu algorithm which takes input feature matrix and:\n",
    "* computing mutual information between columns -- features\n",
    "* computing maximum spanning tree on mutual information matrix. [Hint: Use implementation of **minimum** spanning tree in ```scipy``` on matrix of **negative** mutual information. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list:  ['x->y', 'x->z', 'y->w', 'z->q']\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "def chowliu(features):\n",
    "    mutual_info = mutual_info_all(features)\n",
    "    adjacency_matrix = minimum_spanning_tree(-mutual_info)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def get_edge_list(mat):\n",
    "    f_num = mat.shape[0]\n",
    "    edges = []\n",
    "    for k in range(f_num):\n",
    "        lst = np.nonzero(mat[k,:])[1]\n",
    "        # k is parent, j is child\n",
    "        new_edges = [(k,j) for j in lst]        \n",
    "        edges.extend(new_edges)\n",
    "    return edges\n",
    "        \n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(20)\n",
    "y = x + 0.1*np.random.randn(20)\n",
    "z = -x + 0.1*np.random.randn(20)\n",
    "q = z + 0.1*np.random.randn(20)\n",
    "w = y + 0.1*np.random.rand(20)\n",
    "features = np.asmatrix([x,y,z,q,w]).transpose()\n",
    "adjacency = chowliu(features)\n",
    "names = ['x','y','z','q','w']\n",
    "edges = get_edge_list(adjacency)\n",
    "print('Edge list: ',[(names[i] + '->' + names[j]) for (i,j) in edges])\n",
    "\n",
    "edges = get_edge_list(adjacency)\n",
    "\n",
    "assert(set(edges)== set([(0, 1), (0, 2), (1, 4), (2, 3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sanity check for tree building implementation\n",
    "Edge pairs between ```x,y,z,q,w``` should reflect the dependencies of variables in the code.\n",
    "\n",
    "Since \n",
    "```pyhon\n",
    "y = x + 0.1*np.random.randn(20)\n",
    "```\n",
    "```y``` is a noisy version of ```x``` and we expect to have an edge ```x->y```. Similarly, since\n",
    "```\n",
    "w = y + 0.1*np.random.rand(20)\n",
    "```\n",
    "```w``` is a noisy version of ```y``` and we expect\n",
    "to see edges ```y->w```.\n",
    "\n",
    "\n",
    "\n",
    "If you did everything correctly. You should output\n",
    "\n",
    "```Edge list: ['x->y', 'x->z', 'y->w', 'z->q']```\n",
    "\n",
    "and assert should not fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning class-specific trees\n",
    "We need to train a tree per class. \n",
    "\n",
    "**ToDo**\n",
    "\n",
    "* split training data into class-specific data matrices\n",
    "* run Chow-liu tree algorithm on class-specific subsets and obtain resulting edge list\n",
    "* store resulting edge lists, one per class, in dictionary ```class_edges```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([  2,  17,  19,  22,  23,  27,  31,  36,  44,  47,  48,  53,  61,\n",
      "        62,  65,  69,  72,  73,  76,  78,  87,  88,  93,  94,  95,  96,\n",
      "       101, 109, 133, 146, 149, 153, 159, 164, 165, 168, 171, 175, 176,\n",
      "       178, 179, 190, 194, 197, 199, 203, 207, 221, 226, 227, 228, 229,\n",
      "       235, 236, 242, 247, 250, 252, 259, 273, 281, 296, 305, 306, 316,\n",
      "       321, 326, 337, 340, 343, 347, 351, 364, 367, 369, 372, 378, 382,\n",
      "       383, 396, 400], dtype=int64), 1: array([  4,   5,   9,  12,  14,  18,  20,  21,  24,  29,  32,  33,  38,\n",
      "        39,  45,  49,  66,  68,  82,  85,  98, 104, 108, 110, 111, 112,\n",
      "       113, 116, 119, 128, 130, 137, 140, 141, 142, 147, 163, 170, 187,\n",
      "       188, 193, 196, 200, 202, 204, 211, 217, 219, 220, 223, 230, 238,\n",
      "       243, 249, 267, 278, 282, 291, 294, 299, 303, 307, 308, 312, 319,\n",
      "       322, 328, 331, 333, 336, 341, 357, 358, 360, 361, 362, 363, 393],\n",
      "      dtype=int64), 2: array([ 28,  42,  43,  46,  50,  54,  56,  57,  59,  60,  63,  75,  77,\n",
      "        84,  86,  89,  91,  92, 103, 107, 121, 129, 132, 134, 135, 143,\n",
      "       144, 152, 154, 156, 158, 166, 180, 181, 182, 183, 184, 185, 189,\n",
      "       192, 201, 209, 216, 222, 232, 237, 246, 253, 255, 265, 268, 270,\n",
      "       276, 277, 279, 280, 286, 290, 292, 295, 297, 302, 304, 309, 311,\n",
      "       317, 325, 335, 339, 342, 345, 346, 352, 354, 366, 370, 373, 374,\n",
      "       379, 380, 385, 387, 402], dtype=int64), 3: array([  0,   1,   8,  13,  15,  25,  30,  34,  37,  51,  58,  67,  71,\n",
      "        74,  79,  81,  83,  90, 100, 102, 105, 114, 117, 118, 120, 125,\n",
      "       126, 131, 136, 138, 139, 151, 162, 167, 169, 174, 186, 191, 198,\n",
      "       218, 224, 239, 241, 244, 245, 251, 257, 269, 274, 284, 287, 293,\n",
      "       313, 320, 324, 327, 329, 330, 334, 348, 349, 353, 355, 356, 359,\n",
      "       368, 386, 389, 392, 394, 395, 398, 399], dtype=int64), 4: array([  3,   6,   7,  10,  11,  16,  26,  35,  40,  41,  52,  55,  64,\n",
      "        70,  80,  97,  99, 106, 115, 122, 123, 124, 127, 145, 148, 150,\n",
      "       155, 157, 160, 161, 172, 173, 177, 195, 205, 206, 208, 210, 212,\n",
      "       213, 214, 215, 225, 231, 233, 234, 240, 248, 254, 256, 258, 260,\n",
      "       261, 262, 263, 264, 266, 271, 272, 275, 283, 285, 288, 289, 298,\n",
      "       300, 301, 310, 314, 315, 318, 323, 332, 338, 344, 350, 365, 371,\n",
      "       375, 376, 377, 381, 384, 388, 390, 391, 397, 401], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "def get_label_subsets(train_labels):\n",
    "    label_set = np.unique(train_labels) #get 5 label numbers\n",
    "    label_sample_map = {label: np.where(train_labels == label)[0] for label in label_set} #a label to sample index map\n",
    "    print(label_sample_map)\n",
    "    #?????????????\n",
    "    return label_sample_map\n",
    "        \n",
    "def get_edges_for_each_class(train_features, train_labels):\n",
    "    label_set = np.unique(train_labels) #get 5 label numbers\n",
    "    label_sample_map = get_label_subsets(train_labels)    \n",
    "    class_edges = {}\n",
    "    for label in label_set: \n",
    "        features = train_features[label_sample_map[label],:]\n",
    "        edge_list = get_edge_list(chowliu(features))\n",
    "        class_edges[label] = edge_list\n",
    "    return class_edges\n",
    "    \n",
    "class_edges = get_edges_for_each_class(train_features, train_labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning parameters of a tree-structured Bayes net (3pt)\n",
    "\n",
    "Given a tree structure, we can write out log-likelihood for a Bayes net. In a tree, a node has at most single parent.\n",
    "We will denote the set of edges in a tree as Edges. Hence if node $k$ is parent of node $j$ then the pair $(j,k)$ will be in the set Edges. Note that if edge is $(j,k)$ is in the set Edges, then $(k,j)$ will not be. We will denote the node that has no parents -- root -- as $r$.\n",
    "\n",
    "Log-likelihood is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\Theta) &= \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}}\\left[ \\sum_{(j,k) \\in \\textrm{Edges}} \\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}) + \\log p(x_{i,r}\\mid\\theta_{r})\\right]\\\\\n",
    "&= \\sum_{(j,k) \\in \\textrm{Edges}} \\sum_{i=1}^N\\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}) + \\sum_{i=1}^N\\log p(x_{i,r}\\mid\\theta_{r}) \n",
    "\\end{aligned}.\\tag{1}\n",
    "$$\n",
    "\n",
    "Since $\\theta_{j}$ only appears only in terms involving edge $(j,k)$ we can eliminate other terms and obtain  optimal $\\theta_{j}$ as\n",
    "$$\n",
    "\\theta_j^* = \\mathop{\\textrm{argmax}}_{\\theta_j}\\sum_{i=1}^N\\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}).\n",
    "$$\n",
    "Similarly, the optimal $\\theta_{r}$ is given by\n",
    "$$\n",
    "\\theta_r^* = \\mathop{\\textrm{argmax}}_{\\theta_r}\\sum_{i=1}^N\\log p(x_{i,r} \\mid \\theta_r).\n",
    "$$\n",
    "Put succinctly, all of these optimization problems can be solved separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parameter learning\n",
    "\n",
    "We need to specify how the child variables depend on their parents. Since our features are continuous, we will assume that the conditional probabilities $p(x_j \\mid x_k)$ are Gaussian. Each feature will be modeled as a linear function of its parent with a bias contribution. Since root does not have a parent, it will have just a bias parameter.\n",
    "\n",
    "Specifically,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_j &\\mid x_k,\\theta_j \\sim \\mathcal{N}(\\theta_{j,0} + \\theta_{j,1}x_k,\\sigma^2) \\\\\n",
    "x_r &\\mid \\theta_r \\sim \\mathcal{N}(\\theta_{r,0},\\sigma^2)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n",
    "Log conditional probabilities are given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(x_j\\mid x_k,\\theta_j) &= -\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_j - \\theta_{j,0} - \\theta_{j,1}x_{k})^2\\\\\n",
    "\\log p(x_r\\mid \\theta_r) &= -\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_j - \\theta_{r,0} )^2\n",
    "\\end{aligned}.\n",
    "$$\n",
    "Hence, terms in log-likelihood (Eq.1) involving $\\theta_j$ are\n",
    "$$\n",
    "\\sum_{i=1}^N \\left[-\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_{i,j} - \\theta_{j,0} - \\theta_{j,1}x_{i,k})^2 \\right] \\tag{2}\n",
    "$$\n",
    "and  terms involving $\\theta_r$ are\n",
    "$$\n",
    "\\sum_{i=1}^N \\left[-\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_{i,r} - \\theta_{r,0} )^2 \\right]  \\tag{3}.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain optimal parameters for the model. We will maximize expression in Eq. 2 and Eq. 3 above with regard to parameters $\\theta$.\n",
    "\n",
    "We will not obtain a closed form solution for $\\theta_{j,0}$ and $\\theta_{j,1}$ rather we will iterate updates which fix $\\theta_{j,0}$ to get bets $\\theta_{j,1}$ and then fix $\\theta_{j,1}$ to get best $\\theta_{j,0}$. This approach is called **coordinate ascent**.\n",
    "\n",
    "**ToDo**\n",
    "\n",
    "1. Take derivatives of the Eq. 2 and Eq. 3 above with respect to $\\theta_j$ and $\\theta_r$. Equate them to zero and solve for $\\theta_{j,0},\\theta_{j,1}$ and $\\theta_{r,0}$. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^*_{j,0} &= \\frac{\\sum_{i=1}^N ...}{...}\\\\\n",
    "\\theta^*_{j,1} &= \\frac{\\sum_{i=1}^N ...}{\\sum_{i=1}^N ...}\\\\\n",
    "\\theta^*_{r,0} &= \\frac{\\sum_{i=1}^N ... }{...}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that  $\\theta^*_{j,0}$ depends on $\\theta_{j,1}$ and vice versa. This suggest that we need to iterate these updates until there is no more change in either variable. $\\theta_{r,0}$ can be solved in a single update.\n",
    "2. Implement function that finds optimal $\\theta_j$ given vectors $x_j$ and $x_k$. \n",
    "3. Implement function that finds optimal $\\theta_r$ given vector of $x_r$. Our code assumes that ```r=0```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for theta_{r,0}, r is the root_index (0 in our case)\n",
    "def compute_theta_r( x ):\n",
    "    return np.mean(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for theta_{j, 0}, for link between j and k given theta_j_1. \n",
    "#k is parent, j is child\n",
    "def compute_theta_j_k_0( j, k, x, theta_j_1 ):\n",
    "    return np.mean(x[:,j] - theta_j_1 * x[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for theta_{j, 1}, for link between j and k given theta_j_0. \n",
    "#k is parent, j is child\n",
    "def compute_theta_j_k_1( j, k, x, theta_j_0 ):\n",
    "    return np.sum((x[:, j] - theta_j_0) * x[:, k]) / np.sum(x[:, k]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_r_0:1.0333542751510443 theta_j:[0.4950430649417864, 2.0243162317360426]\n"
     ]
    }
   ],
   "source": [
    "d = 1000\n",
    "x = np.zeros((d,2))\n",
    "x[:,0] = 1.0 + np.random.randn(d,)\n",
    "x[:,1] = 0.5 + 2.0*x[:,0] + np.random.randn(d,)\n",
    "theta_j = [0.0,0.0]\n",
    "\n",
    "theta_r = compute_theta_r( x )\n",
    "for it in range(40):\n",
    "    theta_j[0] = compute_theta_j_k_0( 1, 0, x, theta_j[1] )\n",
    "    theta_j[1] = compute_theta_j_k_1( 1, 0, x, theta_j[0])\n",
    "print(\"theta_r_0:{} theta_j:{}\".format(theta_r,theta_j))    \n",
    "assert(abs(theta_r - 1.0)<0.1)\n",
    "assert(abs(theta_j[0] - 0.5)<0.1)\n",
    "assert(abs(theta_j[1] - 2.0)<0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we check that the code you produced accurately estimates $\\theta_r$ and $\\theta_j$s. In the above case, there are only two variables and the true parameters are $\\theta_r = 1.0$, $\\theta_{1,0} = 0.5$ and $\\theta_{1,1} = 2.0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **[Training the model]** Compute $\\theta^*_j$ and $\\theta^*_r$ for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([  2,  17,  19,  22,  23,  27,  31,  36,  44,  47,  48,  53,  61,\n",
      "        62,  65,  69,  72,  73,  76,  78,  87,  88,  93,  94,  95,  96,\n",
      "       101, 109, 133, 146, 149, 153, 159, 164, 165, 168, 171, 175, 176,\n",
      "       178, 179, 190, 194, 197, 199, 203, 207, 221, 226, 227, 228, 229,\n",
      "       235, 236, 242, 247, 250, 252, 259, 273, 281, 296, 305, 306, 316,\n",
      "       321, 326, 337, 340, 343, 347, 351, 364, 367, 369, 372, 378, 382,\n",
      "       383, 396, 400], dtype=int64), 1: array([  4,   5,   9,  12,  14,  18,  20,  21,  24,  29,  32,  33,  38,\n",
      "        39,  45,  49,  66,  68,  82,  85,  98, 104, 108, 110, 111, 112,\n",
      "       113, 116, 119, 128, 130, 137, 140, 141, 142, 147, 163, 170, 187,\n",
      "       188, 193, 196, 200, 202, 204, 211, 217, 219, 220, 223, 230, 238,\n",
      "       243, 249, 267, 278, 282, 291, 294, 299, 303, 307, 308, 312, 319,\n",
      "       322, 328, 331, 333, 336, 341, 357, 358, 360, 361, 362, 363, 393],\n",
      "      dtype=int64), 2: array([ 28,  42,  43,  46,  50,  54,  56,  57,  59,  60,  63,  75,  77,\n",
      "        84,  86,  89,  91,  92, 103, 107, 121, 129, 132, 134, 135, 143,\n",
      "       144, 152, 154, 156, 158, 166, 180, 181, 182, 183, 184, 185, 189,\n",
      "       192, 201, 209, 216, 222, 232, 237, 246, 253, 255, 265, 268, 270,\n",
      "       276, 277, 279, 280, 286, 290, 292, 295, 297, 302, 304, 309, 311,\n",
      "       317, 325, 335, 339, 342, 345, 346, 352, 354, 366, 370, 373, 374,\n",
      "       379, 380, 385, 387, 402], dtype=int64), 3: array([  0,   1,   8,  13,  15,  25,  30,  34,  37,  51,  58,  67,  71,\n",
      "        74,  79,  81,  83,  90, 100, 102, 105, 114, 117, 118, 120, 125,\n",
      "       126, 131, 136, 138, 139, 151, 162, 167, 169, 174, 186, 191, 198,\n",
      "       218, 224, 239, 241, 244, 245, 251, 257, 269, 274, 284, 287, 293,\n",
      "       313, 320, 324, 327, 329, 330, 334, 348, 349, 353, 355, 356, 359,\n",
      "       368, 386, 389, 392, 394, 395, 398, 399], dtype=int64), 4: array([  3,   6,   7,  10,  11,  16,  26,  35,  40,  41,  52,  55,  64,\n",
      "        70,  80,  97,  99, 106, 115, 122, 123, 124, 127, 145, 148, 150,\n",
      "       155, 157, 160, 161, 172, 173, 177, 195, 205, 206, 208, 210, 212,\n",
      "       213, 214, 215, 225, 231, 233, 234, 240, 248, 254, 256, 258, 260,\n",
      "       261, 262, 263, 264, 266, 271, 272, 275, 283, 285, 288, 289, 298,\n",
      "       300, 301, 310, 314, 315, 318, 323, 332, 338, 344, 350, 365, 371,\n",
      "       375, 376, 377, 381, 384, 388, 390, 391, 397, 401], dtype=int64)}\n",
      "processing class label 0\n",
      "processing class label 1\n",
      "processing class label 2\n",
      "processing class label 3\n",
      "processing class label 4\n"
     ]
    }
   ],
   "source": [
    "def get_thetas_for_each_class(train_features, train_labels, class_edges):\n",
    "    label_sample_map = get_label_subsets(train_labels)\n",
    "    label_set = np.unique(train_labels)\n",
    "    class_thetas = {}\n",
    "    f_num = train_features.shape[1]\n",
    "    for lab in label_set:\n",
    "        print( 'processing class label {}'.format(lab) )\n",
    "        c_samples = train_features[label_sample_map[lab],:]\n",
    "        theta_r = compute_theta_r(c_samples)\n",
    "        c_edge_list= class_edges[lab]\n",
    "        thetas = np.zeros((f_num,2)) #the first column shoud be j_0, and the second column should be j_1\n",
    "        #the first row (thetas[0,:]) is for theta_r\n",
    "\n",
    "        for (k,j) in c_edge_list:        \n",
    "            theta_j_1 = 0\n",
    "            #should do coordinate ascent using the function\n",
    "            #compute_theta_j_k_0 and compute_theta_j_k_1 here\n",
    "            for z in range(20):\n",
    "                theta_j_0 = compute_theta_j_k_0(j,k,c_samples,theta_j_1)\n",
    "                theta_j_1 = compute_theta_j_k_1(j,k,c_samples,theta_j_0)\n",
    "            #set the optimal theta_j_0 and theta_j_1 for this the edge (k, j)\n",
    "            thetas[j, 0] = theta_j_0\n",
    "            thetas[j, 1] = theta_j_1\n",
    "        thetas[0,0] = theta_r   \n",
    "        # root has no parents\n",
    "        thetas[0,1] = np.nan\n",
    "        class_thetas[lab] = thetas\n",
    "    return class_thetas\n",
    "\n",
    "class_thetas = get_thetas_for_each_class(train_features,train_labels,class_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute probability of a feature vector x given a Bayes net's structure and parameters (2pt)\n",
    "**ToDo** <br>\n",
    "1) Implement function that computes $\\log p(x_j\\mid x_k,\\theta_j)$ for specific edge using optimal $\\theta^*_j$. Implement a function that computes $\\log p(x_r\\mid \\theta_r)$ using optimal $\\theta^*_r$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log p(x_j|x_k,\\theta_j), for the edge between j and k, given \\theta_{j, 0} and \\theta_{j,1}\n",
    "# Let sigma^2 =1\n",
    "def compute_lp_j_k( j, k, x, theta_j_0, theta_j_1 ):\n",
    "    return -0.5*np.log(2 * np.pi)- 0.5*(x[j] - theta_j_0 - theta_j_1*x[k])**2\n",
    "\n",
    "# compute log p(x_r|\\theta_r), for the root node, given theta_r. \n",
    "def compute_lp_r( x, theta_r ):\n",
    "    return -0.5*np.log(2 * np.pi)- 0.5*(x[0] - theta_r)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement a function that computes log probability of a sample $\\mathbf{x}$ given a Root, Edges, $\\Theta$\n",
    "$$\n",
    "\\log p(\\mathbf{x} \\mid \\Theta^*) = \\sum_{r \\in \\textrm{Roots}} \\log p(x_r \\mid \\theta^*_r) + \\sum_{(j,k) \\in \\textrm{Edges}} \\log p(x_j \\mid x_k, \\theta^*_{j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lp_x_given_Theta( x, thetas, edges):\n",
    "    lp = compute_lp_r(x, thetas[0,0])    \n",
    "    for (k,j) in edges:  \n",
    "        # k is parent, j is child\n",
    "        lp = lp + compute_lp_j_k(j,k,x,thetas[j,0],thetas[j,1])\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute probabilities that a feature vector x belongs to each class (2pt)\n",
    "Tree augmented Naive Bayes uses following model\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(h = c) &= \\pi_c\\\\\n",
    "p(\\mathbf{x} \\mid \\Theta, h) &= p(\\mathbf{x} \\mid \\Theta_c) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\Theta_c$ are parameters of the Bayes net for feature vector $\\mathbf{x}$ trained on class $c$.\n",
    "\n",
    "**ToDo**\n",
    "<br>\n",
    "1) Use Bayes rule to express\n",
    "$$\n",
    "p(h = c | \\mathbf{x},\\Theta) = \\frac{ p(\\mathbf{x} | \\Theta, h = c)p(h = c) }{ \\sum_{d} p(\\mathbf{x}|\\Theta, h = d)p(h = d) }\n",
    "$$\n",
    "\n",
    "2) Implement a function that takes as input $\\Theta$ and a sample $\\mathbf{x}$ and computes $p(h\\mid\\mathbf{x},\\Theta)$. This function will use the expression you derived above. Note that ```compute_lp_x_given_Theta``` computes log probabilty of $\\mathbf{x}$ for a class specific tree, $\\log p(\\mathbf{x}\\mid h=c,\\Theta)$. We provide code that computes $\\log p(h)$. You just need to put these together and make sure that you use log-sum-exp correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_p_h(train_labels):\n",
    "    label_set = np.unique(train_labels)\n",
    "    log_p_h = np.zeros(len(np.unique(labels)))\n",
    "    for i in label_set:\n",
    "        count = len(np.nonzero(train_labels==i)[0])\n",
    "        log_p_h[i] = np.log( float(count) / float(len(train_labels) ) )\n",
    "    return log_p_h\n",
    "\n",
    "log_p_h = get_log_p_h(train_labels)        \n",
    "\n",
    "def logsumexp(vec):\n",
    "    m = np.max(vec,axis=0)   \n",
    "    return np.log(np.sum(np.exp(vec-m),axis=0))+m\n",
    "\n",
    "\n",
    "def p_h_given_x_theta( x, class_edges, class_thetas, log_p_h ):\n",
    "    \n",
    "    C = len(class_thetas)\n",
    "    lognumerator = np.zeros(C)\n",
    "    \n",
    "    # implement Bayes rule here P(B|A) = P(A|B)P(B)/P(A)\n",
    "    # p(h|x, theta) = p(x,theta|h)p(x,theta)/p(h)?\n",
    "    # compute log-numerators first and then normalize using logsumexp\n",
    "    # there are more compact ways to do the normalization\n",
    "    # feel free to rearrange the code, as long as you return correct\n",
    "    # probabilities\n",
    "    \n",
    "    for i in range(C): \n",
    "        edges = class_edges[i]\n",
    "        thetas = class_thetas[i]        \n",
    "        # LL(p(h)) = log(p(x|Theta, h)) + log(p(h)) = log(p(x|Theta,h)p(h))\n",
    "        #            logsumexp(log(p(x|Theta, h)) + log(p(h)))\n",
    "        lognumerator[i] = compute_lp_x_given_Theta(x,thetas,edges) + log_p_h[i]\n",
    "    \n",
    "    # use logsumexp to compute denominator\n",
    "    numerator = np.exp(lognumerator)\n",
    "    denominator = np.exp(logsumexp(lognumerator))\n",
    "\n",
    "    probs = numerator / denominator\n",
    "    print(numerator, denominator, probs)\n",
    "    assert(np.all(probs >= 0))\n",
    "    assert(np.abs(np.sum(probs)-1.0)<1e-5)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make predictions based on probabilities (1pt)\n",
    "**ToDo** <br>\n",
    "1) On test set, make predictions using parameters ```class_thetas```, tree structures ```class_edges```, ```log_p_h``` as input to function ```p_h_given_x_theta```. Given the function's output, select the most probable class for each sample and store corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(test_features,test_labels,class_edges,class_thetas,log_p_h):\n",
    "    label_set = np.unique(train_labels)\n",
    "    pred_lab = np.zeros((test_features.shape[0], 2)) \n",
    "    #the first column shoud be the predicted label, and the second column should be the probability of that label.\n",
    "    for i in range(test_num):\n",
    "        x = test_features[i,:]    \n",
    "        res =  p_h_given_x_theta(x, class_edges, class_thetas, log_p_h)     \n",
    "        # predicted label\n",
    "        pred_lab[i, 0] = np.argmax(res)\n",
    "        # probability of that label\n",
    "        pred_lab[i, 1] = np.max(res)\n",
    "    return pred_lab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.] 0.0 [nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasnias\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\tasnias\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-e5a5eaa3c36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_edges\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_thetas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_p_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction Accuracy: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_lab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-7bb21d3f2f18>\u001b[0m in \u001b[0;36mevaluate_predictions\u001b[1;34m(test_features, test_labels, class_edges, class_thetas, log_p_h)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mp_h_given_x_theta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_edges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_p_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m# predicted label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpred_lab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-97032d771e00>\u001b[0m in \u001b[0;36mp_h_given_x_theta\u001b[1;34m(x, class_edges, class_thetas, log_p_h)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_lab = evaluate_predictions(test_features,test_labels,class_edges,class_thetas,log_p_h)\n",
    "print(\"Prediction Accuracy: {}\".format(np.mean(pred_lab[:,0]==test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Explore prediction performance (1pt)\n",
    "\n",
    "Next we are going to evaluate how frequently photos from one park get mistaken from photos from another.\n",
    "This information is captured in a confusion matrix. An entry of confusion matrix in $i$th row and $j$th column counts how many photos are in class $i$ but were predicted to be in class $j$. The diagonal of that matrix is the count of correct predictions. Off-diagonal entries are counts of specific types of mistakes. \n",
    "\n",
    "Confusion matrix is not symmetric. Mistaking apples for oranges is not the same as mistaking oranges for apples.\n",
    "\n",
    "To earn this point, you just need to run the cells below and produce the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, rotation_mode='anchor', ha = 'right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_lab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-e52459d24ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_lab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabel_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m plot_confusion_matrix(cnf_matrix, [label_names[i] for i in label_set],\n\u001b[0;32m      5\u001b[0m                       title='Confusion matrix, without normalization')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_lab' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_labels, pred_lab[:, 0])\n",
    "label_set = np.unique(test_labels)\n",
    "plot_confusion_matrix(cnf_matrix, [label_names[i] for i in label_set],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "def gen_err_predicted_fig( test_labels, pred_lab, label_set, label_names, \n",
    "                           test_sample_ids, srcPath='images_5_classes', \n",
    "                          figsize=(10, 10)):\n",
    "    errNameMap = {}\n",
    "    maxErrNameMap = {}\n",
    "    cnt = 0\n",
    "    for lab in label_set:\n",
    "        for plab in label_set:\n",
    "            if lab == plab:\n",
    "                continue\n",
    "            else:\n",
    "                cIdx = np.nonzero(test_labels==lab)[0]\n",
    "                errIdx = cIdx[ np.nonzero(pred_lab[cIdx, 0]==plab)[0] ]\n",
    "                errNameMap[(lab, plab)] = errIdx\n",
    "                if len(errIdx) != 0:\n",
    "                    errProb = np.max( pred_lab[errIdx, 1] )\n",
    "                    errFIdx = errIdx[ np.argmax( pred_lab[errIdx, 1] ) ]\n",
    "                    maxErrNameMap[(lab, plab)] = errFIdx\n",
    "    title_font = {'fontname':'Arial', 'size':'16', \n",
    "                  'color':'black', 'weight':'bold'} \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(25):\n",
    "        rIdx = i // 5\n",
    "        cIdx = i % 5\n",
    "        #print(i, rIdx, cIdx)\n",
    "        ax = plt.subplot(5, 5, i+1)\n",
    "        if rIdx == 0:\n",
    "            ax.xaxis.set_label_position('top') \n",
    "            ax.set_xlabel( label_names[int(label_set[cIdx])].decode('utf-8'), \n",
    "                           rotation=45, rotation_mode='anchor', ha = 'left' )\n",
    "        if cIdx == 0:\n",
    "            ax.yaxis.set_label_position('left')\n",
    "            ax.set_ylabel( label_names[int(label_set[rIdx])].decode('utf-8'),  \n",
    "                           rotation=45, rotation_mode='anchor', ha = 'right' )\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        if (label_set[rIdx], label_set[cIdx]) in maxErrNameMap:\n",
    "            fIdx = maxErrNameMap[(label_set[rIdx], label_set[cIdx])]            \n",
    "            folderName = label_names[test_labels[fIdx]]            \n",
    "            fID = test_sample_ids[fIdx]\n",
    "            #int(fID.split('_')[0]) ]\n",
    "            img=mpimg.imread( srcPath + '/' + folderName.decode('utf-8') + '/' + fID.decode('utf-8') + '.jpg')    \n",
    "            plt.imshow(img)  # The AxesGrid object work as a list of axes.\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, cnf_matrix[rIdx, cIdx],\n",
    "                     **title_font)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_lab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-291648339a84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgen_err_predicted_fig\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_lab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_sample_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_lab' is not defined"
     ]
    }
   ],
   "source": [
    "label_set = np.unique(test_labels)\n",
    "gen_err_predicted_fig( test_labels, pred_lab, label_set, label_names, test_sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
